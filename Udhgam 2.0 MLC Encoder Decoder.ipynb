{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Udhgam 2.0 ML Challenge\n## Team Name: Encoder-Decoder\n\n### Overview & Approach\nThe goal of this competition was to predict a \"prompt quality\" score from obfuscated token sequences without access to the original vocabulary or pre-trained embeddings (like BERT).\n\nTo solve this, we developed a CRNN-Attention Hybrid Architecture that combines:\n\n1) Learned Embeddings: Training fresh embeddings from scratch to capture token relationships.\n\n2) 1D-CNN: To capture local n-gram patterns (phrasing/texture).\n\n3) Bi-GRU: To capture long-range sequential dependencies and flow.\n\n4) Attention Mechanism: To allow the model to dynamically focus on the most critical tokens in the prompt.\n\n5) Meta-Features: Explicitly injecting statistical signals (length, complexity, lexical diversity) into the final layer.\n\nKey Optimization Techniques:\n\n* Pseudo-Labeling: We utilized our previous best submission to generate soft labels for the test set, expanding our training manifold.\n\n* Test-Time Augmentation (TTA): We used Monte Carlo Dropout (inference with dropout enabled) to simulate an ensemble of 10 models, significantly reducing variance.","metadata":{}},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, lr_scheduler\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport gc\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nclass Config:\n    # Paths (Update these to match your environment)\n    TRAIN_PATH = '/kaggle/input/ml-challenge-udhgam-2/train.jsonl'\n    TEST_PATH = '/kaggle/input/ml-challenge-udhgam-2/test.jsonl'\n    \n    # Path to previous best submission for Pseudo-Labeling\n    # This acts as an \"Anchor\" to stabilize training\n    ANCHOR_SUB_PATH = 'best_submission.csv' \n    \n    # Model Architecture\n    MAX_LEN = 128\n    EMBED_DIM = 256\n    HIDDEN_DIM = 256\n    DROPOUT = 0.3\n    \n    # Training Hyperparameters\n    BATCH_SIZE = 64\n    EPOCHS = 6\n    LR = 5e-4\n    N_FOLDS = 5\n    \n    # Robust Seed\n    SEED = 2026 \n    \n    # Inference: Test-Time Augmentation Steps\n    TTA_STEPS = 10 \n    \n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef set_seed(seed):\n    \"\"\"Sets the seed for reproducibility across runs.\"\"\"\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(Config.SEED)\nprint(f\"Running on Device: {Config.DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA LOADING & PRE-PROCESSING","metadata":{}},{"cell_type":"code","source":"# DATA LOADING & PRE-PROCESSING\n\ndef load_data(path):\n    data = []\n    with open(path, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return pd.DataFrame(data)\n\n# 1. Load Raw Data\ntrain_df = load_data(Config.TRAIN_PATH)\ntest_df = load_data(Config.TEST_PATH)\n\n# 2. Pseudo-Labeling\n# We merge the predictions from our previous best model to use as soft targets\nif os.path.exists(Config.ANCHOR_SUB_PATH):\n    print(f\"Loading Anchor Submission for Pseudo-Labeling: {Config.ANCHOR_SUB_PATH}\")\n    anchor_sub = pd.read_csv(Config.ANCHOR_SUB_PATH)\n    test_df = test_df.merge(anchor_sub[['example_id', 'label']], on='example_id', how='left')\n    \n    # Mark real training data vs pseudo data\n    train_df['is_pseudo'] = 0\n    test_df['is_pseudo'] = 1\n    \n    # Combine into one massive dataset\n    full_train_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\nelse:\n    print(\"WARNING: Anchor submission not found. Training on labeled data only.\")\n    full_train_df = train_df\n\n# 3. Meta-Feature Extraction\ndef get_meta_features(df):\n    \"\"\"\n    Extracts statistical features from token sequences.\n    - Length: Proxy for prompt detail.\n    - Unique Count / Ratio: Proxy for lexical diversity.\n    - Start/End Tokens: Often capture framing (e.g., 'Write...', '...in JSON').\n    - Std Dev: Measures the spread of token IDs.\n    \"\"\"\n    feats = []\n    for ids in df['input_ids']:\n        l = len(ids)\n        u = len(set(ids))\n        if l == 0: l = 1 # Safety\n        feats.append([l, u, u/l, ids[0], ids[-1], np.std(ids)])\n    return np.array(feats)\n\nprint(\"Generating Meta Features...\")\nfull_meta = get_meta_features(full_train_df)\ntest_meta_only = get_meta_features(test_df)\n\n# Normalize Meta Features (Critical for Neural Network stability)\nscaler = StandardScaler()\nfull_meta = scaler.fit_transform(full_meta)\ntest_meta_only = scaler.transform(test_meta_only)\n\n# Determine Vocabulary Size\nVOCAB_SIZE = max([max(x) for x in full_train_df['input_ids']]) + 2\nprint(f\"Vocabulary Size: {VOCAB_SIZE}\")\n\n# 4. Dataset Class\nclass UnifiedDataset(Dataset):\n    def __init__(self, df, meta, max_len, is_test=False):\n        self.input_ids = df['input_ids'].values\n        self.meta = meta\n        self.max_len = max_len\n        self.is_test = is_test\n        if not is_test:\n            self.labels = df['label'].values\n            \n    def __len__(self): return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        ids = self.input_ids[idx]\n        # Pad or Truncate to Fixed Length\n        if len(ids) > self.max_len: \n            ids = ids[:self.max_len]\n        else: \n            ids = ids + [0] * (self.max_len - len(ids))\n            \n        return {\n            'input_ids': torch.tensor(ids, dtype=torch.long),\n            'meta': torch.tensor(self.meta[idx], dtype=torch.float),\n            'label': torch.tensor(self.labels[idx], dtype=torch.float) if not self.is_test else torch.tensor(0.0)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture: CRNN-Attention\nThis is the core of our solution. We combined Convolutional layers with Recurrent layers to get the best of both worlds:\n\n* Conv1D: Extracts local features (like n-grams) efficiently.\n\n* Bi-GRU: Captures global context and sequence flow.\n\nAttention: Computes a weighted sum of the hidden states, allowing the model to emphasise important tokens dynamically.","metadata":{}},{"cell_type":"code","source":"class CRNNAttention(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, meta_dim):\n        super().__init__()\n        # 1. Learnable Embeddings\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        \n        # 2. Convolutional Layer (Local Pattern Extraction)\n        self.conv1 = nn.Conv1d(embed_dim, hidden_dim, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        \n        # 3. Recurrent Layer (Sequence Modeling)\n        self.gru = nn.GRU(hidden_dim, hidden_dim, num_layers=2, \n                          batch_first=True, bidirectional=True, dropout=0.2)\n        \n        # 4. Attention Mechanism\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim * 2, 64),\n            nn.Tanh(),\n            nn.Linear(64, 1),\n            nn.Softmax(dim=1)\n        )\n        \n        # 5. Meta-Feature Fusion Head\n        self.meta_fc = nn.Linear(meta_dim, 32)\n        \n        # 6. Final Regressor\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2 + 32, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 1)\n        )\n        \n    def forward(self, input_ids, meta):\n        # Embed: [Batch, Len, Dim]\n        x = self.embedding(input_ids)\n        \n        # Conv: [Batch, Dim, Len]\n        x = x.permute(0, 2, 1)\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = x.permute(0, 2, 1) # Back to [Batch, Len, Dim]\n        \n        # GRU: Output [Batch, Len, Hidden*2]\n        h, _ = self.gru(x)\n        \n        # Attention: Calculate weights for each time step\n        attn_weights = self.attention(h)\n        # Context: Weighted sum of hidden states\n        context = torch.sum(h * attn_weights, dim=1)\n        \n        # Process Meta-Features\n        m = torch.relu(self.meta_fc(meta))\n        \n        # Concatenate Context + Meta\n        combined = torch.cat((context, m), dim=1)\n        \n        # Output Score (Sigmoid ensures 0-1 range)\n        return torch.sigmoid(self.fc(combined)).squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING & INFERENCE UTILS","metadata":{}},{"cell_type":"code","source":"# TRAINING & INFERENCE UTILS\n\ncriterion = nn.SmoothL1Loss(beta=0.01)\n\ndef train_fn(model, loader, optimizer, scheduler, criterion, device):\n    model.train()\n    for batch in loader:\n        ids = batch['input_ids'].to(device)\n        meta = batch['meta'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        output = model(ids, meta)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n# --- MAIN EXECUTION LOOP ---\nkf = KFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\nfinal_preds = np.zeros(len(test_df))\n\nprint(f\"Starting Training with {Config.N_FOLDS}-Fold CV...\")\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(full_train_df)):\n    print(f\"Fold {fold+1}/{Config.N_FOLDS}\")\n    \n    # Create Datasets\n    train_ds = UnifiedDataset(full_train_df.iloc[train_idx], full_meta[train_idx], Config.MAX_LEN)\n    real_test_ds = UnifiedDataset(test_df, test_meta_only, Config.MAX_LEN, is_test=True)\n    \n    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(real_test_ds, batch_size=Config.BATCH_SIZE, shuffle=False)\n    \n    # Initialize Model\n    model = CRNNAttention(VOCAB_SIZE, Config.EMBED_DIM, Config.HIDDEN_DIM, 6).to(Config.DEVICE)\n    optimizer = AdamW(model.parameters(), lr=Config.LR, weight_decay=1e-4)\n    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=Config.LR, steps_per_epoch=len(train_loader), epochs=Config.EPOCHS)\n    \n    # Train\n    for epoch in range(Config.EPOCHS):\n        train_fn(model, train_loader, optimizer, scheduler, criterion, Config.DEVICE)\n    \n    # --- TTA INFERENCE ---\n    # We keep the model in .train() mode to enable Dropout during inference\n    model.train() \n    fold_tta_preds = np.zeros(len(test_df))\n    \n    with torch.no_grad():\n        for t in range(Config.TTA_STEPS):\n            batch_preds = []\n            for batch in test_loader:\n                ids = batch['input_ids'].to(Config.DEVICE)\n                meta = batch['meta'].to(Config.DEVICE)\n                out = model(ids, meta)\n                batch_preds.extend(out.cpu().numpy())\n            fold_tta_preds += np.array(batch_preds)\n            \n    # Average the TTA predictions\n    final_preds += (fold_tta_preds / Config.TTA_STEPS) / Config.N_FOLDS\n    \n    # Cleanup\n    del model, optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\n\nprint(\"Training Complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FINAL BLEND & CALIBRATION\n\nprint(\"Blending Predictions...\")\n\nif os.path.exists(Config.ANCHOR_SUB_PATH):\n    # Weighted Blend: 85% Anchor (Stability) + 15% New TTA (Refinement)\n    # This conservative blend protects against overfitting while adding the TTA gains\n    anchor_preds = pd.read_csv(Config.ANCHOR_SUB_PATH)['label'].values\n    final_blend = (0.85 * anchor_preds) + (0.15 * final_preds)\nelse:\n    final_blend = final_preds\n\n# Calibration: Shift predictions to match the Training Mean\n# This corrects systematic bias (e.g., if model consistently predicts too low)\ntrain_mean = full_train_df[full_train_df['is_pseudo']==0]['label'].mean()\npred_mean = final_blend.mean()\ndiff = train_mean - pred_mean\n\nprint(f\"Calibration Shift: {diff:.5f}\")\nfinal_blend += diff\n\n# Clip to ensure valid range\nfinal_blend = np.clip(final_blend, 0, 1)\n\n# Save Submission\nsubmission = pd.DataFrame({\n    'example_id': test_df['example_id'],\n    'label': final_blend\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission Saved: submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}